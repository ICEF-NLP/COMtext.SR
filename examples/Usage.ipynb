{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Priprema\n",
    "\n",
    "Učitavanje neophodnih biblioteka i inicijalizacija tokenizatora i jezičkog modela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "main_folder = '/'.join(sys.path[0].split('/')[:-1])\n",
    "sys.path.append(main_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/clarinsi/classla-resources/main/resources_2.0.json: 10.5kB [00:00, 2.00MB/s]                                                          \n",
      "2024-03-18 13:10:02 INFO: Downloading these customized packages for language: sr (Serbian)...\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | standard |\n",
      "========================\n",
      "\n",
      "2024-03-18 13:10:02 INFO: Finished downloading models and saved to /Users/lenka/classla_resources.\n",
      "2024-03-18 13:10:02 INFO: Loading these models for language: sr (Serbian):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | standard |\n",
      "========================\n",
      "\n",
      "2024-03-18 13:10:02 INFO: Use device: cpu\n",
      "2024-03-18 13:10:02 INFO: Loading: tokenize\n",
      "2024-03-18 13:10:02 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.ner import ner_model\n",
    "import json\n",
    "import classla\n",
    "from helpers.lemmatizer import load_dictionary, get_lemma, DIALECT_INDEX_EKAVICA, DIALECT_INDEX_IJEKAVICA\n",
    "\n",
    "with open('labels.json', 'r') as label_file:\n",
    "    labels = json.load(label_file)\n",
    "    \n",
    "model = ner_model.NERModel('electra',  'ICEF-NLP/bcms-bertic-comtext-sr-legal-msd-ekavica', \n",
    "                           use_cuda=False, ignore_mismatched_sizes=True, labels=labels)\n",
    "\n",
    "classla.download('sr', processors='tokenize')\n",
    "nlp = classla.Pipeline('sr', processors='tokenize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizacija rečenica\n",
    "\n",
    "Tokenizacija se vrši uz pomoć CLASSLA tokenizatora za srpski jezik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sudija', 'je', 'izdao', 'propis', '1.', 'klase', 'i', '2.', 'reda', '.'], ['Sud', 'je', 'po', 'prijemu', 'tužbe', 'započeo', 'pripreme', 'za', 'glavnu', 'raspravu', 'koje', 'su', 'obuhvatale', 'i', 'dostavu', 'tužbe', 'tuženom', 'na', 'odgovor', 'u', 'skladu', 'odredbi', 'člana', '274', 'Zakona', 'o', 'parničnom', 'postupku', '.'], ['OBAVEZUJE', 'SE', 'tuženi', 'Zorić', 'Andrija', 'iz', 'Beograda', 'ulica', 'Sarajevska', 'br.', '101', 'da', 'tužilji', 'Kostić', 'Mari', 'iz', 'Beograda', 'ul', '.', 'Miše', 'Vujića', 'br.', '2', 'vrati', 'iznos', 'od', '145.000', 'evra', 'u', 'roku', 'od', '15', 'dana', 'od', 'dana', 'prijema', 'pisanog', 'otpravka', 'presude', 'pod', 'pretnjom', 'izvršenja', '.']]\n"
     ]
    }
   ],
   "source": [
    "examples = ['Sudija je izdao propis 1. klase i 2. reda.', \n",
    "            'Sud je po prijemu tužbe započeo pripreme za glavnu raspravu koje su obuhvatale i dostavu tužbe tuženom na odgovor u skladu odredbi člana 274 Zakona o parničnom postupku.',\n",
    "            'OBAVEZUJE SE tuženi Zorić Andrija iz Beograda ulica Sarajevska br. 101 da tužilji Kostić Mari iz Beograda ul. Miše Vujića br. 2 vrati iznos od 145.000 evra u roku od 15 dana od dana prijema pisanog otpravka presude pod pretnjom izvršenja.']\n",
    "\n",
    "\n",
    "classla_tokenized = []\n",
    "\n",
    "for sentence in examples:\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    tokenized_sentence = []\n",
    "    for word in doc.iter_words():\n",
    "        tokenized_sentence.append(word.text)\n",
    "    classla_tokenized.append(tokenized_sentence)\n",
    "\n",
    "\n",
    "print(classla_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Učitavanje rečnika za lematizaciju\n",
    "\n",
    "Kako se ne bi učitavao ceo rečnik u memoriju, učitavaju se samo podaci vezani za reči koje se nalaze u primerima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'srLex_v1.3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m DICTIONARY_PATHS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrLex_v1.3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhrLex_v1.3\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m wordlist \u001b[38;5;241m=\u001b[39m [j \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m classla_tokenized \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m i]\n\u001b[0;32m----> 3\u001b[0m \u001b[43mload_dictionary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDICTIONARY_PATHS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDIALECT_INDEX_EKAVICA\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDIALECT_INDEX_EKAVICA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mICEF-NLP/bcms-bertic-comtext-sr-legal-msd-ekavica\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/COMtext.SR/helpers/lemmatizer.py:17\u001b[0m, in \u001b[0;36mload_dictionary\u001b[0;34m(filepath, wordlist, dialect, model_name)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_dictionary\u001b[39m(filepath, wordlist, dialect, model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBERTic\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     lex_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     lexicon \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     19\u001b[0m     pos_lexicon \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'srLex_v1.3'"
     ]
    }
   ],
   "source": [
    "DICTIONARY_PATHS = ['srLex_v1.3', 'hrLex_v1.3']\n",
    "wordlist = [j for i in classla_tokenized for j in i]\n",
    "load_dictionary(DICTIONARY_PATHS[DIALECT_INDEX_EKAVICA], wordlist, DIALECT_INDEX_EKAVICA, 'ICEF-NLP/bcms-bertic-comtext-sr-legal-msd-ekavica')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Korišćenje modela za morfološko tagovanje i lematizaciju\n",
    "\n",
    "Jezički model za svaki token predvidja morfološku oznaku, uz pomoć koje se vrši pretraga u rečniku i tako se dobija i lema za svaku reč"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list, model_outputs = model.predict(classla_tokenized, split_on_space=False)\n",
    "\n",
    "for word in preds_list[0]:\n",
    "    token = list(word.keys())[0]\n",
    "    tag = list(word.values())[0]\n",
    "    lemma = get_lemma(list(word.values())[0], list(word.keys())[0], DIALECT_INDEX_EKAVICA)['text']\n",
    "    print(token, tag, lemma)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
